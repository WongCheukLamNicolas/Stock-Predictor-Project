# -*- coding: utf-8 -*-
"""alec_code_4p80_group.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XIYKjkeD6hNZ28NT_McVHkBqYIGbMgBE
"""

#RNN implementation - Alec Petrovic code

import yfinance as yf                                                   #financial data from yahoo finance
import numpy as np
from sklearn.preprocessing import MinMaxScaler                          #normalize data from range 0-1
from tensorflow.keras.models import Sequential                          #sequential model type (linear stack of layers)
from tensorflow.keras.layers import SimpleRNN, Dense                    #SimpleRNN layers and Dense for fully connected layer to make final prediction
from sklearn.metrics import mean_squared_error, mean_absolute_error
import math
import matplotlib.pyplot as plt
import pandas as pd                                                     #used to manipulate data frames
import warnings
import random
import tensorflow as tf

warnings.filterwarnings('ignore')   #ignore all warning messages so output is more readable

#set global seed which is changed each run, so I have different runs I can compare
#and use to calculate an average success rate & best success rate
#(setting seed for numpy, python, and tensorflow)
np.random.seed(11)
random.seed(11)
tf.random.set_seed(11)

#all the stocks the RNN is being trained to predict (run one at a time)
stocks = {
    #'AAPL': 'Apple (Global)',
    #'TGT': 'Target (American)',
    #'EOAN.PR': 'E.ON SE (Europe Electric Utility)',
    #'600104.SS': 'SAIC Motor Corp Ltd (China Automaker)',
    '9843.T': 'Nitori Holdings Co Ltd (Japan Home Furnishings)'
}

#parameter values I am experimenting with
lookback_periods = [30, 60]             #how may previous closing prices used to predict the next days closing price
hidden_sizes = [50, 100]                #number of neurons in SimpleRNN layers
epoch_values = [100, 200]               #training epochs
optimizers = ['adamax', 'sgd', 'ftrl']  #optimizers we are testing

NUM_RUNS = 5 #running each configuration of RNN with 5 different seeds

#create a DataFrame to store all results
#each row contains results from one experiment
all_results = pd.DataFrame(columns=[
    'Stock',                    #stock symbol (e.g., 'AAPL')
    'Stock_Name',               #descriptive name of the stock
    'Lookback',                 #lookback period used
    'Hidden_Size',              #number of neurons in each RNN layer
    'Epochs',                   #number of training epochs
    'Optimizer',                #optimization algorithm used
    'Run',                      #run number 1-5
    'Seed',                     #random seed used for run
    'Train_RMSE',               #Root Mean Squared Error on training data
    'Test_RMSE',                #Root Mean Squared Error on testing data
    'Train_MAE',                #Mean Absolute Error on training data
    'Test_MAE',                 #Mean Absolute Error on testing data
    'Train_Direction_Accuracy', #percentage of correct direction predictions (training)
    'Test_Direction_Accuracy',  #percentage of correct direction predictions (testing)
    'Train_Within_5_Percent',   #percentage of predictions within 5% of actual (training)
    'Test_Within_5_Percent',    #percentage of predictions within 5% of actual (testing)
    'Model_Summary'             #text summary of the model architecture
])

#used to plot the best test run at the end of all experiments
#initially set rmse to infinity, and no data
best_predictions = {'test_rmse': float('inf'), 'data': None}

#create sequence for time series prediction
def create_dataset(dataset, look_back=60):
    #X is list of input features, Y is list of target values
    X, Y = [], []

    #iterate through dataset to create overlapping sequences, ensuring each
    #X has a corresponding Y value
    for i in range(len(dataset) - look_back - 1):
        a = dataset[i:(i + look_back)]      #get all lookback sequences
        X.append(a)                         #append all sequences to X, creating lookback period
        Y.append(dataset[i + look_back])    #and next closing price becomes y

    #return list of features & expected outputs
    return np.array(X), np.array(Y)


#calculates a percentage of how often the rnn correctly predicts
#if the stock goes up or down
#actual: array of actual stock closing prices
#predicted: array of the rnns predicted stock closing prices
def calculate_direction_accuracy(actual, predicted):
    #returns 0 if either array empty for some reason
    if len(actual) == 0 or len(predicted) == 0:
        return 0.0

    #also, must be at least 2 data values to calculate a direction
    if len(actual) < 2:
        return 0.0

    correct = 0 #counts the correct direction predictions made by the model
    total = 0   #total # of comparisons between actual & predicted values

    for i in range(1, len(actual)):
        #1 means up, -1 means down, 0 means closing price did not change
        actual_direction = 1 if actual[i] > actual[i - 1] else -1 if actual[i] < actual[i - 1] else 0
        predicted_direction = 1 if predicted[i] > predicted[i - 1] else -1 if predicted[i] < predicted[i - 1] else 0

        #don't punish the model for predicting the price did not change when this is correct
        #update count for correct & total predictions
        if actual_direction != 0 and predicted_direction != 0:
            if actual_direction == predicted_direction:
                correct += 1
            total += 1

    #return percentage of correct predictions (avoiding divide by 0 error)
    return (correct / total * 100) if total > 0 else 0.0

#calculate percentage of predictions within 5% of the actual closing price
def calculate_within_percentage(actual, predicted, threshold_percent=5):
    #does not work if actual & predicted lists are empty
    if len(actual) == 0 or len(predicted) == 0:
        return 0.0

    within_threshold = 0 #counts how many predictions within 5% of closing price

    #calculate percentage error using the standard formula:
    #|(predicted - actual) / actual| * 100
    #and check if this is within the 5% threshold or not
    for a, p in zip(actual, predicted):
        percentage_error = abs((p - a) / a) * 100
        if percentage_error <= threshold_percent:
            within_threshold += 1

    #return percentage of predictions within 5% of actual closing prices
    return (within_threshold / len(actual)) * 100



#train & evaluate a RNN with specified parameters
#returns a dictionary with all results for a specific configuration of the RNN
def train_and_evaluate_model(stock_code, stock_name, look_back, hidden_size, epochs, optimizer_name, seed, run_number):

    #set specific seed for this run (numpy, python, and tensorflow)
    np.random.seed(seed)
    random.seed(seed)
    tf.random.set_seed(seed)

    """
    print(f"\n{'-' * 80}")
    print(f"Processing: {stock_name} ({stock_code})")
    print(f"Parameters: Lookback={look_back}, Hidden Size={hidden_size}, Epochs={epochs}, Optimizer={optimizer_name}")
    print('-' * 80)
    """

    try:
        #download historical stock data for given stock
        print(f"Downloading data for {stock_code}...")
        data = yf.download(stock_code, start="2020-01-01", end="2023-12-08", progress=False)

        #make sure data downloaded successfully
        if data.empty:
            print(f"Warning: No data available for {stock_code}")
            return None

        #use only closing price
        if "Close" in data.columns:
            data = data[["Close"]].copy()
        else:
            print(f"Warning: No 'Close' column for {stock_code}")
            return None

        #normalize the data between 0-1
        scaler = MinMaxScaler()
        data_scaled = scaler.fit_transform(data.values)

        #split data into training and testing sets (80/20)
        train_size = int(len(data_scaled) * 0.8)
        train_data = data_scaled[:train_size]
        test_data = data_scaled[train_size:]

        #create datasets
        #x_train is the seqence of lookback days, y_train is the next days closing price for each sequence
        #(same for testing data)
        X_train, Y_train = create_dataset(train_data, look_back)
        X_test, Y_test = create_dataset(test_data, look_back)

        #reshape input for RNN in format [samples, time_steps, features]
        #samples = sequences, time steps = lookback, features = number of closing prices (features)
        X_train = np.reshape(X_train, (X_train.shape[0], look_back, 1))
        X_test = np.reshape(X_test, (X_test.shape[0], look_back, 1))

        #create sequential RNN (linear stack of layers)
        model = Sequential()

        #this is the first SimpleRNN layer, units = hidden size, returns full sequence of
        #outputs needed for next layer
        model.add(SimpleRNN(units=hidden_size, return_sequences=True, input_shape=(look_back, 1)))

        #second layer similar to first, but only returns last output in sequence
        model.add(SimpleRNN(units=hidden_size))

        #last, fully connected layer of RNN, single neuron which is the models final prediction
        model.add(Dense(1))

        #compile model with specified optimizer

        #default learning rate for adam function, so using it for adamax too
        if optimizer_name == 'adamax':
            from tensorflow.keras.optimizers import Adamax
            optimizer = Adamax(learning_rate=0.001)

        #larger learning rate works better for sgd than adamax, and momentum
        #usually needed to escape local minima
        elif optimizer_name == 'sgd':
            from tensorflow.keras.optimizers import SGD
            optimizer = SGD(learning_rate=0.01, momentum=0.9)

        elif optimizer_name == 'ftrl':
            from tensorflow.keras.optimizers import Ftrl
            optimizer = Ftrl(learning_rate=0.01)

        #now model compiled with correct optimizer & MSE loss function
        model.compile(loss='mean_squared_error', optimizer=optimizer)

        #get model summary as string to reference and compare later
        """
        import io
        from contextlib import redirect_stdout
        f = io.StringIO()
        with redirect_stdout(f):
            model.summary()
        model_summary = f.getvalue()
        """

        #train model (on training data)
        #print(f"Training model...")

        # model.fit() trains the model:
        # X_train, Y_train: training data and labels
        # epochs=epochs: number of training epochs (from parameter)
        # batch_size=64: number of samples per gradient update
        # verbose=0: no progress output during training (keeps output clean)
        history = model.fit(X_train, Y_train, epochs=epochs, batch_size=64, verbose=0)

        #make predictions (i.e. run forward pass through trained network with
        #training data, then testing data)
        train_predict = model.predict(X_train, verbose=0)
        test_predict = model.predict(X_test, verbose=0)

        #inverse transform predictions to original scale (from normalized values back to prices)
        #(this applies the inverse of the MinMax() scaling used for normalization to do this)
        #(also need to reshape Y values to 2D to 'un-normalize', then can undo this)
        train_predict = scaler.inverse_transform(train_predict)
        Y_train_orig = scaler.inverse_transform(Y_train.reshape(-1, 1))
        test_predict = scaler.inverse_transform(test_predict)
        Y_test_orig = scaler.inverse_transform(Y_test.reshape(-1, 1))

        # Calculate evaluation metrics
        # Root Mean Squared Error: square root of average squared differences
        # Mean Absolute Error: average of absolute differences
        train_rmse = math.sqrt(mean_squared_error(Y_train_orig, train_predict))
        test_rmse = math.sqrt(mean_squared_error(Y_test_orig, test_predict))
        train_mae = mean_absolute_error(Y_train_orig, train_predict)
        test_mae = mean_absolute_error(Y_test_orig, test_predict)

        print(f"Results - Train RMSE: {train_rmse:.3f}, Test RMSE: {test_rmse:.3f}, "
              f"Train MAE: {train_mae:.3f}, Test MAE: {test_mae:.3f}")

        #flatten arrays to calculate metrics easier
        Y_train_flat = Y_train_orig.flatten()
        train_predict_flat = train_predict.flatten()
        Y_test_flat = Y_test_orig.flatten()
        test_predict_flat = test_predict.flatten()

        #calculate direction accuracy on training and testing data
        train_dir_acc = calculate_direction_accuracy(Y_train_flat, train_predict_flat)
        test_dir_acc = calculate_direction_accuracy(Y_test_flat, test_predict_flat)

        #calculate predictions within 5% of actual closing price during training and testing
        train_within_5 = calculate_within_percentage(Y_train_flat, train_predict_flat, 5)
        test_within_5 = calculate_within_percentage(Y_test_flat, test_predict_flat, 5)

        #print the results
        print(f"         Train Direction Acc: {train_dir_acc:.1f}%, Test Direction Acc: {test_dir_acc:.1f}%")
        print(f"         Train Within 5%: {train_within_5:.1f}%, Test Within 5%: {test_within_5:.1f}%")

        #check if this is the best run so far (keeping track of best run for final conclusions)
        global best_predictions #making global to modify only in this function

        #update the best run based on rmse, not mae, rmse punishes large errors worse than mae does
        if test_rmse < best_predictions['test_rmse']:
            #store new lowest error & all data needed for plotting at end of all experiments
            best_predictions = {
                'test_rmse': test_rmse,
                'data': {
                    'test_rmse': test_rmse,
                    'stock_code': stock_code,
                    'stock_name': stock_name,
                    'look_back': look_back,
                    'hidden_size': hidden_size,
                    'epochs': epochs,
                    'optimizer': optimizer_name,
                    'seed': seed,
                    'run_number': run_number,
                    'Y_test_actual': Y_test_orig.flatten(),
                    'Y_test_predicted': test_predict.flatten()
                }
            }

        #clean up tensorflow and keras session to free gpu and cpu memory
        #important since I am running many experiments and can easily fill memory
        from tensorflow.keras import backend as K
        K.clear_session()
        tf.compat.v1.reset_default_graph()

        #store all results from this experiment
        result_row = {
            'Stock': stock_code,
            'Stock_Name': stock_name,
            'Lookback': look_back,
            'Hidden_Size': hidden_size,
            'Epochs': epochs,
            'Optimizer': optimizer_name,
            'Run': run_number,
            'Seed': seed,
            'Train_RMSE': train_rmse,
            'Test_RMSE': test_rmse,
            'Train_MAE': train_mae,
            'Test_MAE': test_mae,
            'Train_Direction_Accuracy': train_dir_acc,
            'Test_Direction_Accuracy': test_dir_acc,
            'Train_Within_5_Percent': train_within_5,
            'Test_Within_5_Percent': test_within_5,
            'Model_Summary': f"Model with seed {seed}"
        }

        # Plot results (optional - can comment out if too many plots)
        #plot_results(stock_name, stock_code, look_back, hidden_size, optimizer_name,
        #             Y_train_orig, train_predict, Y_test_orig, test_predict)

        #return the experiments results
        return result_row

    #catch any error which may occur when training model
    except Exception as e:
        print(f"Error processing {stock_code}: {str(e)}")
        return None

#plot training and testing results
#NOT USING ANYMORE, CAN PROBABLY DELETE)
def plot_results(stock_name, stock_code, look_back, hidden_size, optimizer_name,
                 Y_train, train_predict, Y_test, test_predict):
    # Create a figure with 1 row and 2 columns of subplots, 15x5 inches
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))

    # Plot training data on first subplot (axes[0])
    # Plot actual training values as blue line with 70% opacity
    axes[0].plot(Y_train, label='Actual', color='blue', alpha=0.7)
    axes[0].plot(train_predict, label='Predicted', color='red', alpha=0.7)
    axes[0].set_title(
        f'{stock_name} - Training Data\nLookback: {look_back}, Hidden: {hidden_size}, Optimizer: {optimizer_name}')
    axes[0].set_xlabel('Time')
    axes[0].set_ylabel('Stock Price')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # Plot testing data
    axes[1].plot(Y_test, label='Actual', color='blue', alpha=0.7)
    axes[1].plot(test_predict, label='Predicted', color='red', alpha=0.7)
    axes[1].set_title(
        f'{stock_name} - Testing Data\nLookback: {look_back}, Hidden: {hidden_size}, Optimizer: {optimizer_name}')
    axes[1].set_xlabel('Time')
    axes[1].set_ylabel('Stock Price')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()


# Initialize counters for tracking experiments
experiment_count = 0  #total number of experiments
successful_runs = 0   #total number of experiments done successfully (as in no errors)

print(f"Total experiments to run: {len(stocks) * len(lookback_periods) * len(hidden_sizes) * len(epoch_values) * len(optimizers) * NUM_RUNS}")
print("-" * 100)

#iterate through all stocks & run experiments
for stock_code, stock_name in stocks.items():
    print(f"\n{'#' * 100}")
    print(f"STARTING ANALYSIS FOR: {stock_name} ({stock_code})")
    print('#' * 100)

    #running all combination of parameter values for each stock (i.e. running
    #all experiments for each stock)
    for look_back in lookback_periods:
        for hidden_size in hidden_sizes:
            for epochs in epoch_values:
                for optimizer_name in optimizers:

                    print(f"\n{'=' * 80}")
                    print(
                        f"Testing: {stock_name} | Lookback: {look_back} | Hidden: {hidden_size} | Epochs: {epochs} | Optimizer: {optimizer_name}")
                    print(f"Running {NUM_RUNS} iterations with different seeds")
                    print('=' * 80)

                    #store results for this configuration across all runs
                    run_results = []

                    #run each experiment 5 times with different seeds to get statistically accurate data
                    for run in range(1, NUM_RUNS + 1):
                        experiment_count += 1

                        #formula used to generate random seed for each run
                        seed = 11 * run + experiment_count

                        #run the experiment with this seed
                        result = train_and_evaluate_model(
                            stock_code, stock_name, look_back, hidden_size,
                            epochs, optimizer_name, seed, run
                        )

                        #if experiment successful, add to config list & print progress for this run
                        if result is not None:
                            run_results.append(result)
                            successful_runs += 1

                            print(
                                f"  Run {run}/{NUM_RUNS}: Test RMSE = {result['Test_RMSE']:.3f}, Test MAE = {result['Test_MAE']:.3f}")

                    #force garbage collector to free memory after each experiment, since I am running
                    #many experiments at once
                    import gc
                    gc.collect()

                    #get stats for this particular experiment (as long as one run was successful)
                    if run_results:
                        #convert list of dictionaries to DataFrame to do calculations easier
                        config_df = pd.DataFrame(run_results)

                        #calculate statistical metrics
                        avg_test_rmse = config_df['Test_RMSE'].mean()   #average test rmse
                        std_test_rmse = config_df['Test_RMSE'].std()    #standard deviation
                        best_test_rmse = config_df['Test_RMSE'].min()   #most successful run (lowest rmse)
                        best_test_mae = config_df['Test_MAE'].min()     #lowest MAE

                        avg_test_mae = config_df['Test_MAE'].mean()     #average test mae
                        std_test_mae = config_df['Test_MAE'].std()      #standard deviation

                        print(f"\n  Configuration Statistics:")
                        print(f"    Average Test RMSE: {avg_test_rmse:.3f} (±{std_test_rmse:.3f})")
                        print(f"    Best Test RMSE: {best_test_rmse:.3f}")
                        print(f"    Average Test MAE: {avg_test_mae:.3f} (±{std_test_mae:.3f})")
                        print(f"    Best Test MAE: {best_test_mae:.3f}")

                        #append all runs for this experiment to all_results
                        all_results = pd.concat([all_results, config_df], ignore_index=True)

print(f"\n{'*' * 100}")
print(f"EXPERIMENT COMPLETE")
print(f"Total experiments attempted: {experiment_count}")
print(f"Successful runs: {successful_runs}")
print(f"Failed runs: {experiment_count - successful_runs}")
print('*' * 100)

#save detailed results to a csv file to perminantly store
all_results.to_csv('rnn_stock_prediction_detailed_results.csv', index=False)
print(f"\nDetailed results saved to 'rnn_stock_prediction_detailed_results.csv'")

#create & show aggregated results (aggregated being the average results across
#all runs for each experiment)
print("\n" + "=" * 100)
print("AGGREGATED RESULTS (Averaged across runs)")
print("=" * 100)

#grouping results by each experiment (parameter values) and calculating stats for each experiment
#by averaging values for each run of each experiment.
#agg() averages the 4 metrics below for each experiments across all 5 runs
agg_results = all_results.groupby(['Stock', 'Stock_Name', 'Lookback', 'Hidden_Size', 'Epochs', 'Optimizer']).agg({
    'Test_RMSE': ['mean', 'std', 'min', 'max', 'count'],
    'Test_MAE': ['mean', 'std', 'min', 'max'],
    'Test_Direction_Accuracy': ['mean', 'std', 'max'],
    'Test_Within_5_Percent': ['mean', 'std', 'max']
}).round(3)

#agg() function above created multi-dimensional columns that can now be flattened
agg_results.columns = ['_'.join(col).strip() for col in agg_results.columns.values]

#convert grouped columns back to regular ones
agg_results = agg_results.reset_index()

#show the 10 most successful experiments (based off of average rmse across all runs)
print("\nTop 10 configurations by average Test RMSE:")
top_10_avg = agg_results.nsmallest(10, 'Test_RMSE_mean')

#print details of these 10 best runs
for i, (_, row) in enumerate(top_10_avg.iterrows(), 1):
    print(f"\n{i}. {row['Stock_Name']} ({row['Stock']})")
    print(f"   Parameters: Lookback={row['Lookback']}, Hidden={row['Hidden_Size']}, "
          f"Epochs={row['Epochs']}, Optimizer={row['Optimizer']}")
    print(f"   Avg Test RMSE: {row['Test_RMSE_mean']:.3f} (±{row['Test_RMSE_std']:.3f})")
    print(f"   Best Test RMSE: {row['Test_RMSE_min']:.3f}")
    print(f"   Avg Direction Accuracy: {row['Test_Direction_Accuracy_mean']:.1f}%")  # ADD THIS
    print(f"   Avg Within 5%: {row['Test_Within_5_Percent_mean']:.1f}%")  # ADD THIS
    print(f"   Runs: {row['Test_RMSE_count']}")

#save averaged (aggregated) results to a csv file
agg_results.to_csv('rnn_stock_prediction_aggregated_results.csv', index=False)
print(f"\nAggregated results saved to 'rnn_stock_prediction_aggregated_results.csv'")

#display summary of stats for each stock
print("\n" + "=" * 100)
print("SUMMARY BY STOCK (Average across all configurations)")
print("=" * 100)

#loop through each stock & print stats
for stock_code, stock_name in stocks.items():
    #get results for current stock
    stock_data = all_results[all_results['Stock'] == stock_code]

    #print results for this stock if they exist
    if not stock_data.empty:
        print(f"\n{stock_name} ({stock_code}):")
        print(f"  Total successful runs: {len(stock_data)}")
        print(f"  Average Test RMSE across all runs: {stock_data['Test_RMSE'].mean():.3f}")
        print(f"  Std Dev of Test RMSE: {stock_data['Test_RMSE'].std():.3f}")
        print(f"  Best Test RMSE: {stock_data['Test_RMSE'].min():.3f}")
        print(f"  Worst Test RMSE: {stock_data['Test_RMSE'].max():.3f}")

        #calculate success rate based on the median rmse
        #successful run is below the median rmse (better than 50% of runs)
        threshold = stock_data['Test_RMSE'].median()
        success_rate = (stock_data['Test_RMSE'] < threshold).mean() * 100
        print(f"  Success rate (RMSE < median): {success_rate:.1f}%")

#create comparison plots showing variability
print("\n" + "=" * 100)
print("GENERATING VARIABILITY ANALYSIS PLOTS")
print("=" * 100)

#create plots for each stock
for stock_code, stock_name in stocks.items():
    #get current stock
    stock_data = all_results[all_results['Stock'] == stock_code]

    #plot data if it exists
    if not stock_data.empty and len(stock_data) > 1:
        #showing variability of Test RMSE across runs
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        fig.suptitle(f'Performance Variability: {stock_name} ({stock_code})', fontsize=16)

        # 1. Box plot of Test RMSE by optimizer
        #shows distribution of RMSE values for each optimizer
        stock_data.boxplot(column='Test_RMSE', by='Optimizer', ax=axes[0])
        axes[0].set_title('Test RMSE Distribution by Optimizer')
        axes[0].set_ylabel('Test RMSE')
        axes[0].set_xlabel('Optimizer')
        axes[0].grid(True, alpha=0.3)

        # 2. Scatter plot showing all runs
        #shows relation between lookback period & test rmse
        for optimizer in optimizers:
            opt_data = stock_data[stock_data['Optimizer'] == optimizer]
            if not opt_data.empty:
                axes[1].scatter(opt_data['Lookback'], opt_data['Test_RMSE'],
                                label=optimizer, alpha=0.6, s=50)
        axes[1].set_title('All Runs: Lookback vs Test RMSE')
        axes[1].set_xlabel('Lookback Period')
        axes[1].set_ylabel('Test RMSE')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

#final summary of all experiments
print("\n" + "=" * 100)
print("FINAL ANALYSIS SUMMARY")
print("=" * 100)

#print overall rnn stats
print(
    f"\nTotal parameter configurations tested: {len(lookback_periods) * len(hidden_sizes) * len(epoch_values) * len(optimizers)}")
print(f"Total stocks analyzed: {len(stocks)}")
print(f"Runs per configuration: {NUM_RUNS}")
print(f"Total individual experiments: {experiment_count}")
print(f"Overall success rate: {successful_runs / experiment_count * 100:.1f}%")

#find & display the most successful experiment
if not all_results.empty:
    overall_best = all_results.loc[all_results['Test_RMSE'].idxmin()]
    print(f"\nOverall Best Single Run:")
    print(f"  Stock: {overall_best['Stock_Name']} ({overall_best['Stock']})")
    print(f"  Parameters: Lookback={overall_best['Lookback']}, Hidden={overall_best['Hidden_Size']}, "
          f"Epochs={overall_best['Epochs']}, Optimizer={overall_best['Optimizer']}")
    print(f"  Seed: {overall_best['Seed']}, Run: {overall_best['Run']}")
    print(f"  Test RMSE: {overall_best['Test_RMSE']:.3f}")
    print(f"  Test MAE: {overall_best['Test_MAE']:.3f}")
    print(f"  Direction Accuracy: {overall_best['Test_Direction_Accuracy']:.1f}%")  # ADD THIS
    print(f"  Within 5% Accuracy: {overall_best['Test_Within_5_Percent']:.1f}%")  # ADD THIS

#plot the predictions for the best experiment
if best_predictions['data'] is not None:
    print(f"\n{'=' * 100}")
    print("PLOTTING BEST RUN PREDICTIONS")
    print('=' * 100)

    data = best_predictions['data']
    plt.figure(figsize=(12, 6))
    plt.plot(data['Y_test_actual'], label='Actual', color='blue', linewidth=2)
    plt.plot(data['Y_test_predicted'], label='Predicted', color='red', linewidth=2, alpha=0.8)
    plt.title(f"Best Run: {data['stock_name']} ({data['stock_code']})\n"
              f"Test RMSE: {data['test_rmse']:.3f}, Lookback: {data['look_back']}, "
              f"Hidden: {data['hidden_size']}, Optimizer: {data['optimizer']}")
    plt.xlabel('Time Index')
    plt.ylabel('Stock Price ($)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

print("=" * 100)
print("ANALYSIS COMPLETE!")
print("=" * 100)
